{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -0.0000e+00,  2.1793e-26],\n",
       "        [-8.5899e+09,  8.4078e-45,  0.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(2, 3).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3428, 0.8489, 0.9058],\n",
       "        [0.9203, 0.7877, 0.1695]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9778, 0.5408, 0.5480],\n",
       "        [0.0875, 0.5451, 0.9959]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5415,  0.2943,  1.0134],\n",
       "        [-0.9688,  0.6885, -1.7732]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]), 'torch.FloatTensor', torch.float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x.size(), x.shape, x.type(), x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2022), tensor(-0.2022))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x), x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2523)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0337), tensor(-0.0337))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x), x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0730,  1.0763, -0.0052])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/steve/Code/teaching/ProgTut/mnist_pytorch.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/steve/Code/teaching/ProgTut/mnist_pytorch.ipynb#ch0000020?line=0'>1</a>\u001b[0m y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mcuda()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "y = x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3334, -1.1825,  0.3157],\n",
       "        [-0.0730,  1.0763, -0.0052]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 4).float().requires_grad_(True)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/distiller/project/pytorch/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "y.sum().grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(32,128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layer1(x)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32]) <class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([128]) <class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([10, 128]) <class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([10]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param.shape, type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1070, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/773vphv51bdclptjzptm_4km0000gn/T/ipykernel_22307/2553623551.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2336e-04,  1.1817e-04,  1.1297e-04,  ..., -2.7387e-05,\n",
      "         -3.2586e-05, -3.7784e-05],\n",
      "        [ 4.0683e-03,  4.0683e-03,  4.0683e-03,  ...,  4.0683e-03,\n",
      "          4.0683e-03,  4.0683e-03],\n",
      "        [ 4.6966e-03,  1.0115e-02,  1.5534e-02,  ...,  1.6184e-01,\n",
      "          1.6725e-01,  1.7267e-01],\n",
      "        ...,\n",
      "        [ 2.7773e-03,  2.7779e-03,  2.7786e-03,  ...,  2.7959e-03,\n",
      "          2.7966e-03,  2.7972e-03],\n",
      "        [-2.6626e-04, -1.4633e-04, -2.6400e-05,  ...,  3.2117e-03,\n",
      "          3.3316e-03,  3.4516e-03],\n",
      "        [ 2.4058e-05,  2.4054e-05,  2.4050e-05,  ...,  2.3934e-05,\n",
      "          2.3929e-05,  2.3925e-05]])\n",
      "tensor([-0.0017, -0.0018,  0.0095,  0.0017,  0.0046, -0.0064, -0.0030, -0.0081,\n",
      "         0.0018, -0.0042,  0.0047, -0.0006,  0.0036,  0.0058,  0.0036,  0.0020,\n",
      "         0.0004, -0.0040,  0.0008, -0.0047, -0.0015, -0.0033, -0.0004,  0.0083,\n",
      "        -0.0083, -0.0032, -0.0093,  0.0025,  0.0011, -0.0017, -0.0022,  0.0009,\n",
      "        -0.0085, -0.0046,  0.0029,  0.0024,  0.0026,  0.0076, -0.0068, -0.0027,\n",
      "         0.0036,  0.0083, -0.0002, -0.0034, -0.0016, -0.0068,  0.0003, -0.0018,\n",
      "         0.0045, -0.0032,  0.0032, -0.0027, -0.0125, -0.0002,  0.0096, -0.0042,\n",
      "         0.0029,  0.0015,  0.0053, -0.0030,  0.0003,  0.0060, -0.0085,  0.0013,\n",
      "        -0.0035,  0.0044,  0.0061, -0.0043, -0.0004, -0.0003,  0.0019, -0.0002,\n",
      "        -0.0105, -0.0054, -0.0015,  0.0072, -0.0014,  0.0051, -0.0084, -0.0091,\n",
      "         0.0082, -0.0022,  0.0064,  0.0006,  0.0023,  0.0010, -0.0037, -0.0065,\n",
      "        -0.0058, -0.0033, -0.0091,  0.0004,  0.0086, -0.0004,  0.0041, -0.0017,\n",
      "         0.0026, -0.0074, -0.0031, -0.0031,  0.0029, -0.0054,  0.0096,  0.0058,\n",
      "        -0.0114,  0.0056,  0.0034, -0.0033,  0.0124, -0.0112, -0.0076, -0.0089,\n",
      "        -0.0034,  0.0050, -0.0049, -0.0028, -0.0065,  0.0027, -0.0053, -0.0068,\n",
      "         0.0082, -0.0005, -0.0021, -0.0072, -0.0036,  0.0051,  0.0024, -0.0045])\n",
      "tensor([[-0.2573,  0.0255, -0.0464,  ...,  0.0363,  0.0250,  0.0359],\n",
      "        [ 0.0468,  0.0144,  0.0275,  ...,  0.0206,  0.0225,  0.0204],\n",
      "        [ 0.0457,  0.0146,  0.0275,  ...,  0.0211,  0.0230,  0.0209],\n",
      "        ...,\n",
      "        [ 0.0533,  0.0176,  0.0323,  ...,  0.0249,  0.0270,  0.0246],\n",
      "        [ 0.0980,  0.0265,  0.0541,  ...,  0.0382,  0.0422,  0.0377],\n",
      "        [-0.1101, -0.0265, -0.0979,  ..., -0.0959, -0.1061, -0.0943]])\n",
      "tensor([-0.2198,  0.0682,  0.0675, -0.2005,  0.0969,  0.0753,  0.1032,  0.0791,\n",
      "         0.1376, -0.2075])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Check that the parameters now have gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1070, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8122, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7342, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6636, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5414, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4880, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4387, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3927, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3497, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3092, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2710, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2353, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1432, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1171, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0929, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0704, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0494, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0298, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0113, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9774, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9618, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9469, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9328, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9193, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9064, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8822, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8709, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8600, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8495, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8394, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8296, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8202, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8112, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8024, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7858, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7778, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7702, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7627, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7555, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7486, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7418, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7352, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7288, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7226, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7166, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7108, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7051, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6995, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6837, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6738, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6690, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6643, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6597, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6552, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6509, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6466, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6424, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6343, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6303, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6265, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6227, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6190, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6153, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6117, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6082, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6048, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6014, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5980, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5948, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5915, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5884, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5853, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5822, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5792, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5762, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5704, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5676, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5648, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5620, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5593, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5567, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5540, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5514, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5489, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5463, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5438, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5414, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5365, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5342, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5318, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5295, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5272, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5250, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5228, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5206, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5184, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5162, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5141, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5120, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5099, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5079, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5058, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5038, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5018, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4999, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4979, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4960, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4922, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4903, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4884, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4866, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4848, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4830, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4812, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4794, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4776, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4759, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4742, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4708, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4691, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4674, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4658, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4641, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4625, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4609, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4593, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4577, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4561, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4546, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4530, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4515, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4499, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4484, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4469, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4454, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4439, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4424, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4410, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4395, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4381, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4366, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4352, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4338, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4324, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4310, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4296, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4282, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4269, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4255, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4241, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4228, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4215, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4201, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4188, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4175, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4162, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4149, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4136, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4123, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4110, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4098, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4085, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4073, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4060, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4048, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4035, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4023, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4011, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3999, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3987, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3975, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3963, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3951, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3927, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3915, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3904, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3892, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3881, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3869, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3858, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3846, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3835, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3824, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3812, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3801, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3790, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3779, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3768, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3757, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3746, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3724, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3714, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3703, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3692, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3682, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3671, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3660, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3650, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3639, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3629, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3619, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3608, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3598, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3588, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3578, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3567, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3557, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3547, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3537, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3527, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3517, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3507, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3497, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3488, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3478, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3468, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3458, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3448, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3439, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3429, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3420, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3410, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3401, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3391, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3382, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3372, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3363, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3353, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3344, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3335, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3326, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3316, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3307, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3298, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3289, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3280, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3271, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3262, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3253, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3244, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3235, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3226, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3217, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3208, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3200, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3191, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3182, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3173, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3165, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3156, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3147, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3139, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3130, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3122, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3113, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3105, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3096, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3088, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3079, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3071, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3063, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3054, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3046, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3038, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3030, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3021, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3005, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2997, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2989, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2981, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2973, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2965, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2957, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2949, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2917, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2909, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2901, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2894, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2878, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2870, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2863, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2855, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2847, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2840, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2832, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2825, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2817, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2810, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2802, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2795, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2780, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2772, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2765, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2757, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2743, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2714, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2707, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2699, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2692, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2685, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2678, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2671, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2664, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2657, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2650, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2643, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2636, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2629, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2622, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2615, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2608, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2601, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2594, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2587, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2581, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2574, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2567, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2560, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2554, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2547, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2540, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2534, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2527, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2520, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2514, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2507, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2501, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2494, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2488, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2481, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2475, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2468, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2462, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2455, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2449, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2442, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2436, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2430, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2423, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2417, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2411, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2405, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2398, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2392, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2386, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2380, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2374, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2367, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2361, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2355, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2349, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2343, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2337, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2331, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2325, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2319, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2313, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2307, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2301, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2295, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2289, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2283, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2277, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2272, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2266, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2260, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2254, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2248, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2243, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2237, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2231, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2225, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2220, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2214, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2208, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2203, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2197, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2192, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2186, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2180, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2175, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2169, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2164, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2158, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2153, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2147, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2142, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2137, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2131, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2126, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2120, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2115, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2110, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2104, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2099, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2094, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2088, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2083, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2078, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2073, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2068, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2062, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2057, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2052, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2047, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2042, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2037, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2032, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2027, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2021, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2016, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2011, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2006, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1996, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1986, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1982, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1977, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1972, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1967, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1962, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1957, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1952, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1947, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1943, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1928, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1924, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1919, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1914, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1909, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1905, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1900, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1895, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1891, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1882, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1877, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1872, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1868, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1863, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1859, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1854, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1850, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1845, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1841, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1836, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1832, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1827, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1823, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1819, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1814, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1810, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1805, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1801, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1797, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1792, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1788, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1784, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1779, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1775, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1771, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1767, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1762, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1754, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1746, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1742, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1737, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1729, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1717, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1713, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1709, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1705, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1701, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1697, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1693, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1689, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1685, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1681, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1677, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1673, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1669, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1665, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1661, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1657, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1653, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1649, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1646, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1642, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1638, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1634, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1630, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1627, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1623, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1619, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1615, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1611, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1608, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1604, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1600, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1597, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1593, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1589, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1586, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1582, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1578, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1575, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1571, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1568, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1564, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1560, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1557, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1553, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1550, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1546, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1543, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1539, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1536, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1532, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1529, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1525, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1522, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1518, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1515, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1511, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1508, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1505, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1501, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1498, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1495, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1491, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1488, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1484, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1481, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1478, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1475, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1471, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1468, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1465, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1461, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1458, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1455, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1452, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1449, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1445, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1442, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1439, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1436, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1433, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1429, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1426, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1423, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1420, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1417, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1414, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1411, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1408, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1404, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1401, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1398, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1395, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1392, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1389, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1386, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1380, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1377, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1374, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1371, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1368, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1365, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1362, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1359, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1357, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1354, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1351, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1348, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1345, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1342, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1339, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1336, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1333, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1331, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1328, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1325, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1322, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1319, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1316, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1314, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1311, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1305, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1303, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1300, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1297, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1294, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1292, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1289, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1286, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1284, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1281, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1278, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1275, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1273, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1270, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1267, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1265, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1262, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1260, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1257, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1254, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1252, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1249, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1247, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1244, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1241, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1239, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1236, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1234, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1231, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1229, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1226, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1224, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1221, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1219, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1216, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1214, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1211, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1209, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1206, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1204, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1201, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1199, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1197, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1194, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1192, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1189, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1187, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1184, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1182, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1180, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1177, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1175, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1173, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1170, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1168, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1166, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1163, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1161, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1159, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1156, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1154, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1152, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1149, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1147, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1145, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1143, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1140, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1138, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1136, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1134, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1131, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1129, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1127, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1125, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1123, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1120, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1118, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1116, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1114, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1112, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1109, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1107, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1105, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1103, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1101, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1099, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1097, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1094, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1092, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1090, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1086, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1084, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1082, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1080, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1078, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1076, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1074, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1071, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1069, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1067, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1063, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1061, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1059, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1055, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1053, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1051, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1049, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1047, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1045, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1043, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1041, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1039, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1037, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1036, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1034, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1028, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1020, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1018, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1016, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1007, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0996, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0992, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0990, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0989, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0966, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0960, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0957, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0952, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0948, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0945, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0943, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0928, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0927, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0923, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0917, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0915, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0914, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0912, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0911, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0909, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0907, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0904, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0901, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0900, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0898, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0896, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0890, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0889, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0887, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0883, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0880, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0875, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0871, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0868, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0865, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0863, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0862, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0860, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0855, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0852, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0850, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0843, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0842, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0839, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0834, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0832, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0828, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0827, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0821, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0820, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0817, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0816, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0812, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0809, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0808, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0807, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0799, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0798, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0795, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0791, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0787, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0784, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0779, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0771, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0770, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0769, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0763, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0762, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0761, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0753, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0752, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0751, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0749, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0748, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0747, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0740, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0739, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0736, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0728, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0727, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0725, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0723, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0719, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0716, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0713, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0712, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0710, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0709, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0706, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0703, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0697, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0695, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0694, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0693, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0691, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0690, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0689, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0687, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0683, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0682, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0681, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0680, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0679, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0677, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "# In a training loop, we should perform many GD iterations.\n",
    "n_iter = 1000\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9481, -1.4883, -1.7687,  0.3211, -1.7742, -1.6009, -1.3723, -1.5065,\n",
      "         -1.3583,  4.0613],\n",
      "        [ 0.4477, -1.4396, -1.4378,  6.1054, -1.2568, -1.3690, -1.3046, -1.3322,\n",
      "         -1.2807,  3.5762],\n",
      "        [ 1.9429, -1.4073, -1.3275,  3.1912, -1.2864, -1.4613, -1.4186, -1.4819,\n",
      "         -1.3147,  5.7723]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['layer1.weight', 'layer1.bias', 'layer3.weight', 'layer3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of keys to weights using `state_dict`\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(),'test.t7')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.t7'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download using tensorflow's keras\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28]), torch.Size([10000]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ddc8beb0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train.cpu().numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = self.X[index].float().reshape(-1)  # flatten input to 1D\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8 if torch.cuda.is_available() else 0 \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    loader_args = dict(shuffle=True, batch_size=256, num_workers=num_workers, pin_memory=True)\n",
    "else:\n",
    "    loader_args = dict(shuffle=True, batch_size=64)\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **loader_args)\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **loader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(self, size_list):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.size_list = size_list\n",
    "        layers = []\n",
    "        for i in range(len(size_list) - 2):\n",
    "            layers.append(torch.nn.Linear(size_list[i], size_list[i+1]))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(size_list[-2], size_list[-1]))\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP([784, 256, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.6010991137113366 Time:  1.5142881870269775 s\n",
      "Testing Loss:  0.23001872079601143\n",
      "Testing Accuracy:  93.86 %\n",
      "====================\n",
      "Training Loss:  0.17382219839078594 Time:  1.4538118839263916 s\n",
      "Testing Loss:  0.20230218895954216\n",
      "Testing Accuracy:  94.33 %\n",
      "====================\n",
      "Training Loss:  0.16147767579970337 Time:  1.4976067543029785 s\n",
      "Testing Loss:  0.19275677901215163\n",
      "Testing Accuracy:  95.57 %\n",
      "====================\n",
      "Training Loss:  0.15502203193960834 Time:  1.4669489860534668 s\n",
      "Testing Loss:  0.18249313689317484\n",
      "Testing Accuracy:  95.47 %\n",
      "====================\n",
      "Training Loss:  0.15835566082064545 Time:  1.4363429546356201 s\n",
      "Testing Loss:  0.24243782043373982\n",
      "Testing Accuracy:  95.24000000000001 %\n",
      "====================\n",
      "Training Loss:  0.14927374951338876 Time:  1.4931590557098389 s\n",
      "Testing Loss:  0.2550597030352089\n",
      "Testing Accuracy:  95.17 %\n",
      "====================\n",
      "Training Loss:  0.15360962421659863 Time:  1.5303921699523926 s\n",
      "Testing Loss:  0.2034646226893992\n",
      "Testing Accuracy:  95.92 %\n",
      "====================\n",
      "Training Loss:  0.14168071359766946 Time:  1.4728069305419922 s\n",
      "Testing Loss:  0.24940808023515923\n",
      "Testing Accuracy:  95.28 %\n",
      "====================\n",
      "Training Loss:  0.12822953812950788 Time:  1.4509892463684082 s\n",
      "Testing Loss:  0.2606611732403884\n",
      "Testing Accuracy:  95.91 %\n",
      "====================\n",
      "Training Loss:  0.1299670436808244 Time:  1.4265961647033691 s\n",
      "Testing Loss:  0.28591875543029666\n",
      "Testing Accuracy:  95.67 %\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "    Train_loss.append(train_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(Train_loss, label='train')\n",
    "plt.plot(Test_loss, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(Test_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9d41da3d759eab0d4ebf46483aa2525ee9e8e52c6441d9f13cc168fd5745a39"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('teaching-kL1iKbCK')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
