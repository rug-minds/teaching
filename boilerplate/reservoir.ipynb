{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/teaching-kL1iKbCK/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x(filename, debug=True):\n",
    "    # returns full data array: (N, T=var, F=12)\n",
    "    with open(filename, 'r') as f:\n",
    "        blocks = f.read().strip().split('\\n\\n')\n",
    "    x = [\n",
    "        np.array([list(map(float, e.split())) for e in block.split('\\n')])\n",
    "        for block in blocks\n",
    "    ]\n",
    "    lens = [len(e) for e in x]\n",
    "    if debug:\n",
    "        print(f'shape: (N={len(x)}, T={min(lens)}-{max(lens)}, F={len(x[0][0])})')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y_flat(sizes, debug=True):\n",
    "    # returns flat output: (N, )\n",
    "    sizes = list(map(int, sizes.split()))\n",
    "    y = []\n",
    "    for idx, size in enumerate(sizes):\n",
    "        y += [idx+1] * size\n",
    "    if debug:\n",
    "        print(len(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y(sizes, debug=True):\n",
    "    # returns one-hot encoded, but without time dimension: (N, C=9)\n",
    "    sizes = list(map(int, sizes.split()))\n",
    "    y = []\n",
    "    for idx, size in enumerate(sizes):\n",
    "        y += np.array([[1. if i == idx else 0. for i in range(9)]] * size)\n",
    "    if debug:\n",
    "        print(f'({len(y)}, {len(y[0])})', end=' ')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_time_dimension(y_train, time_dims, debug=True):\n",
    "    # inflate single vector along given time dimension\n",
    "    y = []\n",
    "    for idx in range(len(y_train)):\n",
    "        y.append(\n",
    "            np.array([y_train[idx] for _ in range(time_dims[idx])])\n",
    "        )\n",
    "    if debug:\n",
    "        lens = [len(e) for e in y]\n",
    "        print(f'(N={len(y)}, T={min(lens)}-{max(lens)}, C={len(y[0][0])})')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (N=270, T=7-26, F=12)\n",
      "test  shape: (N=370, T=7-29, F=12)\n"
     ]
    }
   ],
   "source": [
    "print('train ', end='')\n",
    "x_train = load_x('data/ae.train')\n",
    "print('test  ', end='')\n",
    "x_test = load_x('data/ae.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (270, 9) (N=270, T=7-26, C=9)\n",
      "test  (370, 9) (N=370, T=7-29, C=9)\n"
     ]
    }
   ],
   "source": [
    "print('train ', end='')\n",
    "y_train = load_y('30 30 30 30 30 30 30 30 30')\n",
    "y_train = inflate_time_dimension(y_train, time_dims=[e.shape[0] for e in x_train])\n",
    "print('test  ', end='')\n",
    "y_test = load_y('31 35 88 44 29 24 40 50 29')\n",
    "y_test = inflate_time_dimension(y_test, time_dims=[len(e) for e in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_res = 100\n",
    "N_inp = 12\n",
    "N_out = 9\n",
    "rhoW_target = 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservoir with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: (100, 13), W: (100, 100), Wout: (9, 113)\n"
     ]
    }
   ],
   "source": [
    "Win = (jax.random.uniform(key, (N_res, 1+N_inp)) - 0.5) * 1.\n",
    "W = (jax.random.uniform(key, (N_res, N_res)) - 0.5) * 1.\n",
    "rhoW = np.max(np.absolute(np.linalg.eig(W)[0]))\n",
    "W *= rhoW_target / rhoW\n",
    "Wout = (jax.random.uniform(key, (N_out, 1+N_inp+N_res)) - 0.5) * 1.\n",
    "print(f'Win: {Win.shape}, W: {W.shape}, Wout: {Wout.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(u, Win, W, Wout=None, x_init=np.zeros((N_res,))):\n",
    "    # u: (T, F)\n",
    "    T = u.shape[0]\n",
    "    F = u.shape[1]\n",
    "    assert F == N_inp, 'input shape mismatch'\n",
    "    X, Y = [], []\n",
    "    # X: (1+F+N_res, T)\n",
    "    # Y: (N_out, T)\n",
    "    x = x_init.copy()\n",
    "    for t in range(u.shape[0]):\n",
    "        x = np.tanh(\n",
    "            np.dot(Win, np.concatenate((np.ones(1,), u[t]))) + np.dot(W, x)\n",
    "        )\n",
    "        full_state = np.concatenate((np.ones(1,), u[t], x))\n",
    "        X.append(full_state)\n",
    "        if Wout is not None:\n",
    "            y = np.dot(Wout, full_state)\n",
    "            Y.append(y)\n",
    "        # generative mode:\n",
    "        # u = y\n",
    "        # predictive mode:\n",
    "        # u = data[trainLen+t+1]\n",
    "    # when returning, need to transpose the data arrays, such that dimensions: (T, x)\n",
    "    if Wout is None:\n",
    "        return x, np.array(X).T\n",
    "    else:\n",
    "        return x, np.array(X).T, np.array(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: (20, 12), x: (100,), X: (113, 20)\n"
     ]
    }
   ],
   "source": [
    "x, X = forward(x_train[0], Win, W)\n",
    "print(f'u: {x_train[0].shape}, x: {x.shape}, X: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 12), (20, 9))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape, y_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservoir training\n",
    "\n",
    "TODO: confirm equations are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-8\n",
    "Wout_rc = np.linalg.solve(\n",
    "    np.dot(X, X.T) + reg * np.eye(1+N_inp+N_res),\n",
    "    np.dot(X, y_train[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wout_rc_ = np.dot(\n",
    "    np.dot(y_train[0].T, X.T),\n",
    "    np.linalg.inv(\n",
    "        np.dot(X, X.T) + reg*np.eye(1+N_inp+N_res)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop training\n",
    "\n",
    "The current `forward` function will not work with JAX, we need to use JAX primitives to implement it (rather than iterating over lists). The `forward_bp` function uses the `jax.lax.scan` method to apply the forward pass over a list of inputs, while carrying over the network's state from one time step to the next (see the API reference [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan)).\n",
    "\n",
    "We then apply a softmax to the output layer and compute the cross-entropy loss. \n",
    "\n",
    "JAX does gradient descent automatically using the `grad` function. With that, we can define our gradient update step.\n",
    "\n",
    "TODO: implement batching (handle variable sequence lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # normalized softmax\n",
    "    x_norm = x - np.max(x)\n",
    "    x_exp = np.exp(x_norm)\n",
    "    return x_exp / np.sum(x_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_bp(params, u, x_init=np.zeros((N_res,))):\n",
    "    \"\"\" Loop over the time steps of the input sequence\n",
    "    u:      (time, features)\n",
    "    x_init: (n_res, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout = params\n",
    "    x = x_init.copy()\n",
    "\n",
    "    def apply_fun_scan(params, x, ut):\n",
    "        \"\"\" Perform single step update of the network.\n",
    "        x:  (n_res, )\n",
    "        ut: (features, )\n",
    "        \"\"\"\n",
    "        Win, W, Wout = params\n",
    "        x = np.tanh(\n",
    "            np.dot(Win, np.concatenate((np.ones(1,), ut))) + np.dot(W, x)\n",
    "        )\n",
    "        y = softmax(np.dot(\n",
    "            Wout,\n",
    "            np.concatenate((np.ones(1,), ut, x))\n",
    "        ))\n",
    "        return x, y\n",
    "\n",
    "    f = functools.partial(apply_fun_scan, params)\n",
    "    _, Y = jax.lax.scan(f, x, u)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 9)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = forward_bp((Win, W, Wout), x_train[0])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true):\n",
    "    # cross entropy loss (see Bishop's Pattern Recognition book, page 209).\n",
    "    y_pred = forward_bp(params, u)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1)) / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.3605596, dtype=float32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss((Win, W, Wout), x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "dWin, dW, dWout = jax.grad(loss)((Win, W, Wout), x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 100),\n",
       " DeviceArray(-0.0001335, dtype=float32),\n",
       " DeviceArray(0.05244131, dtype=float32))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW.shape, dW.mean(), dW.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, x, y_true, step_size=1e-2):\n",
    "    grads = jax.grad(loss)(params, x, y_true)\n",
    "    return [\n",
    "        w - step_size * dw\n",
    "        for w, dw in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Win_, W_, Wout_ = update((Win, W, Wout), x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservoir with TF\n",
    "\n",
    "TODO: make this work for ragged tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn_keras = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=[N_inp], ragged=True),\n",
    "    tfa.layers.ESN(units=N_res, connectivity=0.1, leaky=1.0, spectral_radius=rhoW_target),\n",
    "    tf.keras.layers.Dense(9, activation='softmax'),\n",
    "])\n",
    "esn_keras.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['mse', 'accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ragged.constant([[np.array(a) for a in e] for e in x_train])\n",
    "x = x.to_tensor(default_value=0., shape=[None, N_inp])\n",
    "y = tf.ragged.constant([e.tolist() for e in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = esn_keras.fit(\n",
    "    x, y,\n",
    "    batch_size=64,\n",
    "    epochs=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9d41da3d759eab0d4ebf46483aa2525ee9e8e52c6441d9f13cc168fd5745a39"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('teaching-kL1iKbCK')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
